\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}

% latex-math includes as needed
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

% Lecture title always has to be there
\title{Algorithms and Data Structures}

\begin{document}

\titlemeta{Numerics}{Numerical Stability}{figure_man/stability.png}
{
  \item Stability of algorithms
}

%\lecture{CIM1 Statistical Computing}

%\section{Stability}

\begin{vbframe}{Stability of algorithms}

\begin{itemize}
\item The condition of a problem describes the "error amplification" of input errors.
\item The condition is given by the problem (or the data) and we have usually \textbf{no} influence on it.
\item In practice, a numerical task is often divided into smaller subproblems, i.e. an algorithm

$$
f = f_m \circ f_{m-1} \circ ... \circ f_1
$$

is performed.
\item We can influence the way \textbf{how} we solve the problem, i.e. the algorithm.
\end{itemize}

\framebreak

At best, the amplification of the error is not much greater than the condition of the problem. The algorithm is called \textbf{stable}.

\lz

If the problem is well-conditioned, then a \textbf{stable algorithm} should also be found for calculation.

\lz

If either the problem is ill-conditioned \textbf{or} the algorithm is unstable, the result should be questioned.

\framebreak

There are two concepts that can be used to investigate the stability of an algorithm:

\begin{itemize}
\item In the \textbf{forward analysis}, the error is estimated and accumulated for each partial result.
\item In the \textbf{backward analysis}, the result is interpreted as an exactly calculated result for disturbed data. For which input $\xt$ would $f$ return the same result?
$$
\tilde f(x) = f(\xt)?
$$
If $|\xt - x|$ is small, the algorithm is backward stable.
\end{itemize}


\end{vbframe}

\begin{vbframe}{Examples of stability}

\textbf{Example 1:}

We would like to calculate the smallest absolute root of the quadratic equation $p(x) = x^2 - 2 b x + c = 0$, using the solution formula

$$
x_0 = b - \sqrt{b^2 - c}
$$

In this case, $(b, c)$ are given by the problem and the root is the desired result. The algorithm should map $(b, c)$ to the root value $x_0$ ($f: (b, c)\mapsto x_0$).

\lz

For simplification, $b \in \R$ is fixed and we examine the condition of the problem at $c$ using the formula

$$
\kappa = \frac{|c|}{|f(c)|} |f'(c)|.
$$

\begin{eqnarray*}
f'(c) &=& \frac{1}{2} (b^2 - c)^{-1/2} = \frac{1}{2\sqrt{b^2 - c}} \\
\kappa &=& \bigg|\frac{c}{2\sqrt{b^2 - c}(b - \sqrt{b^2 - c})}\bigg| \\
&=& \frac{1}{2}\bigg| \frac{c (b + \sqrt{b^2 - c})}{\sqrt{b^2 - c}(b - \sqrt{b^2 - c})(b + \sqrt{b^2 - c})}\bigg| \\
&=& \frac{1}{2}\bigg|\frac{b + \sqrt{b^2 - c}}{\sqrt{b^2 - c}}\bigg| 
\end{eqnarray*}

Especially for $c \ll b^2$ the problem is well-conditioned.

\framebreak

Let
\vspace{0.2cm}
\footnotesize

\begin{verbatim}
b = 400000
c = - 1.234567890123456
\end{verbatim}
%\col

\normalsize
\vspace{0.2cm}
Then the problem is well-conditioned with $\kappa =  0.999999999998071$

% <<>>=
% kappa = 1 / 2 * (b + sqrt(b^2 - c)) / (sqrt(b^2 - c))
% kappa
% @


\lz
But note that $\kappa$ gives the condition for the function, not the implementation!
\footnotesize
\vspace{0.2cm}
\begin{verbatim}
sqrt(b^2 - c); b
## [1] 400000.0000015432
## [1] 4e+05
\end{verbatim}
%\col

\vspace{0.2cm}
\normalsize
We expect a loss of significance. 
We lose $11$ decimal places in accuracy. 
Therefore a maximum of $16 - 11 = 5$ decimals should be correct in the result.

\framebreak

The following formula provides a stable implementation:

$$
y = \frac{c}{z} \qquad z = b + \sqrt{b^2 - c}
$$
\lz
\footnotesize
\begin{verbatim}
# Stable alternative for x0
x0.instable = b - sqrt(b^2 - c)
x0.stable = c / (b + sqrt(b^2 - c))
\end{verbatim}

\vspace{0.1cm}
\begin{verbatim}
c(x0.instable, x0.stable)
## [1] -1.543201506137848e-06 -1.543209862651343e-06
\end{verbatim}

\vspace{0.1cm}
\begin{verbatim}
p = function(x) x^2 - 2 * b * x + c
c(p(x0.instable), p(x0.stable))
## [1] -6.685210796275598e-06 0.000000000000000e+00
\end{verbatim}

\normalsize

\framebreak

\textbf{Example 2:}

The logistic function

$$
f(x) = \frac{1}{1 + \exp(-x)} = \frac{\exp(x)}{1 + \exp(x)}
$$

and its generalization, the softmax function,

$$
s(\xv)_k = \frac{\exp(x_k)}{\sum_j \exp(x_j)}
$$


play an important role in statistical applications and machine learning:

\begin{itemize}
\item (logistic) distribution function
\item logistic regression
\item activation function in neural networks
\end{itemize}

\framebreak

Large absolute values of $x_j$ can result in an

\begin{itemize}
\item \textbf{Underflow} (large negative values $\to 0$)
\item \textbf{Overflow} (large positive values $\to \infty$)
\end{itemize}
\lz
\footnotesize

\begin{verbatim}
exp(-500)
## [1] 7.124576406741286e-218

.Machine$double.xmin
## [1] 2.225073858507201e-308

exp(-1000)
## [1] 0

exp(1000)
## [1] Inf
\end{verbatim}

\normalsize

\framebreak
Overflow is avoided by the following equivalent equation

$$
s(\xv)_k = \frac{\exp(x_k - b)}{\sum_j \exp(x_j - b)}, \qquad b := \max_i x_i
$$

\lz
\footnotesize
\begin{verbatim}
softmax = function(x) exp(x) / sum(exp(x))
x = c(990, 1000, 999)
\end{verbatim}

\vspace{0.1cm}

\begin{verbatim}
softmax(x) # Instable version (Overflow)
## [1] NaN NaN NaN
\end{verbatim}

\vspace{0.1cm}

\begin{verbatim}
softmax(x - 1000) # stable version without Overflow
## [1] 3.318890658198521e-05 7.310343155951328e-01
## [3] 2.689324954982852e-01
\end{verbatim}

\normalsize

\framebreak

Another problem is underflow in the numerator. A naive implementation of the log softmax function leads to problems.
\lz
\lz
\footnotesize
\begin{verbatim}
x = c(800, 0.0001, -800)

log.softmax = function(x) {
  r = sapply(x, function(t) exp(t) / sum(exp(x)))
  log(r)
}

log.softmax(x)
## [1] NaN -Inf -Inf
\end{verbatim}

\normalsize
\framebreak
Stable alternative implementation:

$$
\log s(\xv)_k = x_k - b - \log \sumjn \exp(x_j - b), \qquad b := \max_i x_i
$$
\lz
\footnotesize
\begin{verbatim}
log.softmax2 = function(x) {
  b = max(x)
  logsum = b + log(sum(exp(x - b)))
  sapply(x, function(t) t - logsum)
}

log.softmax2(x)
## [1] 0.0000 -799.9999 -1600.0000
\end{verbatim}

\normalsize

% Zwei Möglichkeiten zur Berechnung von $f(x)$:
% \begin{enumerate}
% \item  $f_1(x) = \frac{1}{1 + e^{-x}}$
% \item $f_2(x) = \frac{e^x}{1 + e^x}$
% \end{enumerate}
%
%  \framebreak

% Das Problem ist gut konditioniert:
%
% \begin{eqnarray*}
% f^{\prime}(x) &=& - \frac{e^x}{(e^x + 1)^2} \\[0.2cm]
% \kappa &\approx& |x| \cdot \frac{|e^x (e^x + 1)^{-2}|}{1 - (1 + e^{-x})^{-1}} =
%       x \cdot \frac{e^x (e^x + 1)^{-2}}{\frac{(1 + e^{-x}) - 1}{(1 + e^{-x})}} \\
%   &=& x \cdot \frac{e^x (1 + e^{-x})}{(e^x + 1)^2 e^{-x}} =
%       x \cdot \frac{e^{2x}(1 + e^{-x})}{(e^x + 1)^2} \\
%   &=& x \cdot \frac{e^{2x} + e^x}{(e^x + 1)^2} = x \cdot \frac{e^x(e^x + 1)}{(e^x + 1)^2} \\
%   &=& x \cdot \frac{e^x}{e^x + 1} \approx x
% \end{eqnarray*}
%
% \framebreak

% \begin{vbframe}{Fehleranalyse}
% Zur Analyse von Algorithmen in Maschinenarithmetik ist es nützlich
% die Fehlerrelation der Maschinenzahlen  $|\xt - x| \le \epsm |x|$ umzuschreiben
% $$
% \xt = x(1 + \delta) , \quad \mbox{ wobei } |\delta| \leq \epsm \; .
% $$
% Ein Algorithmus besteht aus einer Reihe von arithmetischen Operationen, und jede hat ihre jeweilige Konditionszahl
% $$
% \frac{\|f_j(\mathbf{x(1+\delta)})-
% f_j(\xv)\|}{\|f_j(\xv)\|} \leq \kappa_j \
% \mathbf{\delta}
% $$
% {\bf Achtung:}  $\delta$ nun relativer Fehler

% \lz

% Die Fehleranalyse eines Algorithmus beruht dann auf der sukzessiven
% Berechnung der Fehlerfortpflanzung, wobei Fehler hvherer Ordnung
% typischerweise vernachldssigt werden.

% Salopp: $\qquad f_j(\xt) = f_j(x) (1 + \kappa_j \delta)$
% \end{vbframe}


% \begin{vbframe}{Beispiel zur Stabilitdt: Fehleranalyse}
% $g(t) = 1 - (1 + e^{-t})^{-1}$ besteht aus
% $$
% f_1(z) = e^{-z}, \quad f_2(z) = 1 + z, \quad f_3(z) = 1/z, \quad f_4(z) = 1 - z
% $$
% mit Konditionszahlen

% \lz

% $\kappa_1 = z$, $\kappa_2 = z/(1+z) \leq 1$, $\kappa_3 = 1$, $\kappa_4 = |z/(1-z)|$

% \lz

% Problematisch ist speziell $\kappa_4$ wenn $z \approx 1$.

% \framebreak

% Fehlerfortpflanzungsanalyse:
% \begin{eqnarray*}
% f_1(t+\delta t) &\approx& f_1(t) (1 + \kappa_1 \epsm) = e^{-t} (1 + t \epsm) \\
% f_2[ f_1(t) (1 + t \epsm) ] &\approx& f_2(f_1(t)) (1 + \kappa_2  t \epsm) \leq
%    (1 + e^{-t}) (1 +  t \epsm) \\
% f_3[f_2( f_1(t))(1 + t \epsm)] &\approx& f_3(f_2(f_1(t))) (1 + \kappa_3 t \epsm) \\
%   &=& (1 + e^{-t})^{-1} (1 +  t \epsm) \\
% f_4[f_3(f_2( f_1(t)))(1 + t \epsm)] &\approx&  g(t) (1 + \kappa_4 t \epsm) \approx
%  g(t)(1 + e^x t) \epsm
% \end{eqnarray*}
% Der Fehler $t \epsm$ ist unvermeidbar, aber der Faktor $ \kappa_4 \approx e^x$ ist aufgrund der schlechten Formulierung des Algorithmus.

% \framebreak

% Zum Vergleich
% $g(t) = e^{-t}/(1 + e^{-t})$ besteht aus
% $$
% f_1(z) = e^{-z}, \quad f_2(z) = 1 + z, \quad f_5(z_1,z_2) = z_1 / z_2
% $$
% Konditionszahlen für $f_1$ und $f_2$ wurden schon behandelt. Die
% Division ist ebenso unproblematisch wie die Multiplikation (sofern
% $z_2 \neq 0$):
% $$
% \frac{(z_1+\delta_1)/(z_2+\delta_2) - z_1/z_2}{z_1/z_2} = \frac{z_2 \delta_1 - z_1 \delta_2}{z_1(z_2 + \delta_2)} \approx  \frac{\delta_1}{z_1} - \frac{\delta_2}{z_2}
% $$
% wobei die letze Approximation gilt so lange $|\delta_2| \ll |z_2|$. Je nach Wahl der Vektornorm gilt für die Konditionszahl auf jeden Fall $\kappa_5 < 2$.

% \framebreak

% Fehlerfortpflanzungsanalyse:
% \begin{eqnarray*}
% f_1(t+\delta t) &\approx& f_1(t) (1 + \kappa_1 \epsm) = e^{-t} (1 + t \epsm) \\
% f_2[ f_1(t) (1 + t \epsm) ] &\approx& f_2(f_1(t)) (1 + \kappa_2  t \epsm) \\
%   &\leq& (1 + e^{-t}) (1 +  t \epsm) \\
% f_5[f_1(t + \delta), f_2( f_1(t))(1 + t \epsm)] &\approx& f_5[f_1(t), f_2( f_1(t))] (1 + \kappa_5 t \epsm) \\
%   &\approx& g(t) (1 + 2 t \epsm)
% \end{eqnarray*}
% Die Fehleranalyse zeigt eindeutig dass die zweite Formel sich
% wesentlich besser verhdlt, der Fehler des Algorithmus bewegt sich
% in der Grv_enordnung der Konditionszahl von $g(t)$.

% \framebreak

%<<include=FALSE>>=
%dlogis2 = function(t) 1 - 1/(1 + exp(-t))
%dlogis3 = function(t) exp(-t)/(1 + exp(-t))
%c(dlogis(20), dlogis2(20),dlogis3(20))
%c(dlogis(30), dlogis2(30),dlogis3(30))
%c(dlogis(50), dlogis2(50),dlogis3(50))
%@


\end{vbframe}

\endlecture
\end{document}
